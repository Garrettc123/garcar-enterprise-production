name: ğŸ’¾ Database Backup

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:

jobs:
  backup:
    name: ğŸ“¦ Backup Production Database
    runs-on: ubuntu-latest
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ—„ï¸ Backup PostgreSQL Database
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          PGPASSWORD=${{ secrets.DB_PASSWORD }} pg_dump \
            -h ${{ secrets.DB_HOST }} \
            -U postgres \
            -d revenue_aggregator \
            -F c \
            -f backup_${TIMESTAMP}.dump

      - name: â˜ï¸ Upload to S3/Cloud Storage
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: ğŸ“¤ Sync Backup to S3
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          aws s3 cp backup_${TIMESTAMP}.dump \
            s3://garcar-backups/database/ \
            --storage-class GLACIER

      - name: ğŸ§¹ Clean Old Backups (Keep 30 days)
        run: |
          CUTOFF_DATE=$(date -d '30 days ago' +%Y-%m-%d)
          aws s3 ls s3://garcar-backups/database/ | \
            while read -r line; do
              BACKUP_DATE=$(echo $line | awk '{print $1}')
              if [[ "$BACKUP_DATE" < "$CUTOFF_DATE" ]]; then
                BACKUP_FILE=$(echo $line | awk '{print $4}')
                aws s3 rm s3://garcar-backups/database/$BACKUP_FILE
              fi
            done
